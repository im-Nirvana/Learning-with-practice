{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e236ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Try and exept for macros\n",
    "try:\n",
    "        \n",
    "except Exception as e:\n",
    "            exc_type,exc_obj,exc_tb = sys.exc_info()\n",
    "\n",
    "            logging.exception(f\"The following  exception has occured in the function 'ReadParameter' at line :{exc_tb.tb_lineno}\")\n",
    "            logging.exception(\"-------------------------------------------------------------------------------------------------\")\n",
    "            logging.exception(e)\n",
    "            logging.exception(\"-------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def formatting(num,width):\n",
    "        '''Function for formatting the numerical columns with a certain width.'''\n",
    "\n",
    "        a=float(str(num)[:width+1])\n",
    "        number_dec = str(a).split('.')[1]\n",
    "        K=len(number_dec)-1\n",
    "        res = float(\"{{:.{}f}}\".format(K).format(a))\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def upcase_header(df):\n",
    "        \"\"\"function to change the dataframe header to uppercase.\"\"\"\n",
    "\n",
    "        df.columns=df.columns.str.upper()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c3c0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "Data = pd.DataFrame({'A':['Hai','Hai',\"Hai\",'Hello','Hello','Good','Happy','Happy'],\n",
    "                    'B':['ABJ','ABJ',\"VBK\",'VNK','VNK','ABJ','MNK','KOO'],\n",
    "                    'C':[755,725,753,7545,90,45,45,46]})\n",
    "\n",
    "\n",
    "Data['flag'] = np.where(Data.A != Data.A.shift(1), 1, 0)\n",
    "#here if the for the first valuee in tthe column falg column will set to 1 else 0\n",
    "\n",
    "\n",
    "for last value\n",
    "Data['flag'] = np.where(Data.A != Data.A.shift(-1), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (first.&xkey. and last.&xkey.);\n",
    "globals()[str(ods)].loc[ globals()[str(ods)].groupby(str(xkey),as_index=False).nth([0,-1]).index, 'flag' ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the first alphabet occurence of a column.\n",
    "anyalpha(upc)\n",
    "\n",
    "for i in range (1,len(product_upcs_t)+1):\n",
    "    temp= re.search(r'[a-z]', product_upcs_t['upc'][i], re.I)\n",
    "    if temp is not None:\n",
    "        product_upcs_t['first_alpha'][i] = temp.start()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shift\n",
    "data frag01;\n",
    "retain fhilf;\n",
    "set fragen;\n",
    "by frage;\n",
    "if first.frage then fhilf=sum(fhilf,1);\n",
    "run;\n",
    "\n",
    "frag01=fragen.copy(deep=True)\n",
    "frag02=frag01.groupby(['FRAGE']).first().reset_index(drop=False)\n",
    "frag02['FHILF']=(frag02.index)+1\n",
    "dic=dict(zip(frag02['FRAGE'],frag02['FHILF']))\n",
    "frag01['FHILF']=frag01['FRAGE'].map(dic)\n",
    "logger.info(f\"fragen has %s rows and %s columns\" %(len(fragen.axes[0]),len(fragen.axes[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency\n",
    "proc freq data = fragen noprint;\n",
    " tables frage/out = anz_ant;\n",
    "run;\n",
    "\n",
    "\n",
    "# Creating table anz_ant from fragen\n",
    "col_name=fragen.columns.tolist()\n",
    "for i in range(0,1):  \n",
    "    freq=fragen.groupby(col_name[i]).size().reset_index(name=\"COUNT\")\n",
    "    Percent=((freq['COUNT']/freq['COUNT'].sum())*100).round(2)\n",
    "    Cummulative_Frequency=freq['COUNT'].cumsum()\n",
    "    Cummulative_Percent=((freq['COUNT'].cumsum()/freq['COUNT'].sum())*100).round(2)\n",
    "    freq['PERCENT']=Percent\n",
    "    freq['CUMMULATIVE_FREQUENCY']=Cummulative_Frequency\n",
    "    freq['CUMMULATIVE_PERCENT']=Cummulative_Percent\n",
    "anz_ant=freq.copy(deep=True)\n",
    "anz_ant=anz_ant[['FRAGE','COUNT','PERCENT']]\n",
    "logger.info(f\"anz_ant has %s rows and %s columns\" %(len(anz_ant.axes[0]),len(anz_ant.axes[1])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "proc freq data = hh_&&region&i.._&kw. noprint;\n",
    " tables bebau/out=haben_&&region&i.._&kw.;\n",
    "run;\n",
    "\n",
    "\n",
    "# Finding frequency\n",
    "s=globals()['hh_'+str(region[i])+'_'+str(kw)]['BEBAU'].value_counts().sort_index()\n",
    "\n",
    " globals()['haben_'+str(region[i])+'_'+str(kw)]= pd.DataFrame({\n",
    "            'COUNT': s.values,\n",
    "            'BEBAU': s.index,\n",
    "            'PERCENT': ((s.values/s.values.sum())*100).round(2)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%minmax(a.mbd,b.mbd)\n",
    "tmp1['mbdSG']=np.vectorize(minmax)(tmp1['MBD'],tmp1['mbd_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACHAT.loc[ACHAT['NB_TP_o'] <= 0 , 'NB_TP_o'] = ACHAT['rb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacompy\n",
    "tmp=pd.read_excel(r'C:\\Users\\1949524\\OneDrive - TCS COM PROD\\Documents\\SAS to Python\\Development\\UC117\\LaCooperativaCad\\sas_intermediates\\SELDESCORTA.xlsx')\n",
    "compare = datacompy.Compare(SELDESCORTA,\n",
    "tmp,\n",
    "join_columns= ['EANCOD1'], #You can also specify a list of columns\n",
    "abs_tol=0,\n",
    "rel_tol=0,\n",
    ")\n",
    "print(compare.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797683fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d44273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPBL(DESCRIPCION)\n",
    "TODO['SHORTDES']=TODO['DESCRIPCION'].str.replace(r'\\s+',' ',regex=True)\n",
    "\n",
    "#COMPRESS(EANCODE1)\n",
    "TODO['EANCODE1'].astype(str).str.strip()\n",
    "\n",
    "#TRANSLATE(EAN,'0',' ')\n",
    "TODO['EANCODE1'].astype(str).str.replace(' ','0',regex=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d428d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "statements\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "try:\n",
    "    logger.info('Starting....')\n",
    "    main()\n",
    "    logger.info('Build complete.....')\n",
    "except:\n",
    "    print('error')\n",
    "    logger.error(f'Failed..\\n {traceback.format_exc()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writer\n",
    "with pd.ExcelWriter( r\"C:\\1924686_aditi\\SAS_files\\8_UC124.xlsx\" ) as writer:\n",
    "        sumarDATOMES.to_excel(writer, sheet_name=\"sumarDATOMES\", index=False)\n",
    "        CHECK.to_excel(writer, sheet_name=\"check\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d064d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging with file handler\n",
    "#************Configure logger*******************************************\n",
    "#get config file\n",
    "logging.config.fileConfig('logger_config.cfg')\n",
    "# create logger\n",
    "filename='QC_Puppies_kitten_HP'\n",
    "logger = logging.getLogger(filename)\n",
    "#create custom formatter\n",
    "formatter = logging.Formatter('%(asctime)s : %(name)s : %(levelname)s - %(message)s')\n",
    "#create handler\n",
    "if os.path.exists(f\"logs/{filename}.log\"):\n",
    "    os.remove(f\"logs/{filename}.log\")\n",
    "fh = logging.FileHandler(f\"logs/{filename}.log\")\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "#*****************************************************************************\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "pgm = '12_act_scanner_prueba_mon'\n",
    "\n",
    "# Create a custom logger\n",
    "\n",
    "\n",
    "logger = logging.getLogger(pgm)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "c_handler = logging.StreamHandler()\n",
    "c_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "f_handler = logging.FileHandler(f'{pgm}_log.log',mode = 'w')\n",
    "f_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters and add it to handlers\n",
    "\n",
    "c_format = logging.Formatter('%(name)s - %(levelname)s - line_no:%(lineno)d- %(message)s')\n",
    "\n",
    "#f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - line_no:%(lineno)d - %(message)s')\n",
    "\n",
    "f_format = logging.Formatter('[%(asctime)s]-%(name)s- p%(process)s- {%(pathname)s:-line_no:%(lineno)d}- %(levelname)s -funcName:%(funcName)s - %(message)s','%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "\n",
    "c_handler.setFormatter(c_format)\n",
    "\n",
    "f_handler.setFormatter(f_format)\n",
    "\n",
    "# Add handlers to the logger\n",
    "\n",
    "#logger.addHandler(c_handler)\n",
    "\n",
    "logger.addHandler(f_handler)\n",
    "\n",
    "logger.info(f'{pgm} Execution starts here')\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "#Creating DB Connection for SQL\n",
    "db_conn=create_engine('sqlite://',echo=False)\n",
    "# connection=create_engine(r\"sqlite:///\",echo=False)\n",
    "# db_conn = connection.raw_connection()\n",
    "\n",
    "# cur = db_conn.cursor()\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with buffer\n",
    "FileName = \"load_idc\"+str(per)\n",
    "\n",
    "#Creating log file\n",
    "#logging.captureWarnings(True)\n",
    "logger=logging.getLogger(FileName)\n",
    "logging.basicConfig(filename=config.srceloc+FileName+\".log\",format='%(asctime)s : %(name)s : %(lineno)s : %(message)s', datefmt='%d-%b-%y %H:%M:%S', filemode = 'w')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.info(\"Begining of execution\")\n",
    "buffer=io.StringIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGGING\n",
    "def st(df,name):\n",
    "    \n",
    "    \"\"\"To call logging.info easier\n",
    "        df : Variable\n",
    "        name : name as string\"\"\"\n",
    "    \n",
    "    if str(type(df)) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "        rows = str(len(df))\n",
    "        columns = str(len(df.columns))\n",
    "\n",
    "        s = 'Dataframe '+name+' created, with '+rows+' rows and '+columns+' columns'\n",
    "    else:\n",
    "        s = 'Variable '+name+' resolves to '+str(df)\n",
    "    \n",
    "    return s\n",
    "\n",
    "log = logging.info\n",
    "log(st(CounProj,'CounProj')); log(st(n,'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad713bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config ini file\n",
    "cfg = ConfigParser()\n",
    "cfg.read('config.ini')\n",
    "\n",
    "histo = cfg.get('QC_Puppies_kitten_HP','histo')\n",
    "\n",
    "pet = cfg.get('QC_Puppies_kitten_HP','pet')\n",
    "\n",
    "dv = cfg.get('QC_Puppies_kitten_HP','dv')\n",
    "\n",
    "\n",
    "\n",
    "# set sysdate variable to current date if test_date not defined in config file else set sysdate = test_date\n",
    "try:\n",
    "    if cfg.has_option('QC_Puppies_kitten_HP','test_date'):\n",
    "        temp_date = cfg.get('QC_Puppies_kitten_HP','test_date')\n",
    "        sysdate = datetime.datetime.strptime(temp_date,\"%d%b%Y\")\n",
    "    else:\n",
    "        sysdate = datetime.datetime.now()\n",
    "    logger.info(f'sysdate = {sysdate}')\n",
    "except:\n",
    "    print('Please check the format of the test_date')\n",
    "#***********************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546593a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date time format\n",
    "date_object = datetime.datetime.strptime(date,\"%d/%m/%Y\")\n",
    "date_object.strftime(\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83349bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging final\n",
    "# creating log file for load_df1_202150\n",
    "logging.basicConfig(filename='load_df1_202150.log',\n",
    "filemode='w+',\n",
    "format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "datefmt='%d-%b-%y %H:%M:%S',\n",
    "level=logging.DEBUG,force=True)\n",
    "handler = logging.FileHandler('load_df1_202150.log', 'w+')\n",
    "logger=logging.getLogger('load_df1_202150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7dbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=log_file_name,\n",
    "                        filemode='a',format=\">{%(asctime)s} [PID:%(process)d] (%(name)s)  [%(funcName)s():%(lineno)s]  >>%(levelname)s: %(message)s\",\n",
    "                        datefmt='%d-%b-%y %H:%M:%S',\n",
    "                        level=logging.DEBUG, force = True)\n",
    "logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead51fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #uncomment in CIS\n",
    "    \n",
    "#     if os.path.isfile(mfullfilename+'.gz'):\n",
    "#         a=\"zcat\"+ mfullfilename+'.gz'+\"|sed s'/\\r//g'>\"+mfullfilename+\".csv\"\n",
    "#         os.system(a)\n",
    "#         cols=['FILECODE','CDSRV','DTENC','NUTCKCLT','CDVTE','CDTCKVNT','QTVTE','MTVTE','ANNULATION','VLPDS','MTREM','Filler']\n",
    "#         colspec_list=[(0,2),(2,7),(7,13),(13,20),(20,33),(33,36),(36,45),(45,54),(54,55),(55,64),(64,73),(74,80)]\n",
    "#         FR1=pd.read_fwf(FR1,colspecs=colspec_list,names=cols,dtype=str,skiprows=2)\n",
    "#     else:\n",
    "#         b=\"cat\"+ mfullfilename+\"|sed s'/\\r//g'>\"+mfullfilename+\".csv\"\n",
    "#         os.system(b)\n",
    "#         cols=['FILECODE','CDSRV','DTENC','NUTCKCLT','CDVTE','CDTCKVNT','QTVTE','MTVTE','ANNULATION','VLPDS','MTREM','Filler']\n",
    "#         colspec_list=[(0,2),(2,7),(7,13),(13,20),(20,33),(33,36),(36,45),(45,54),(54,55),(55,64),(64,73),(74,80)]\n",
    "#         FR1=pd.read_fwf(FR1,colspecs=colspec_list,names=cols,dtype=str,skiprows=2)\n",
    "\n",
    "#     comment in CIS\n",
    "    \n",
    "    if(os.path.exists(mfullfilename+'.gz')):\n",
    "        FR1=gzip.open('mfullfilename.gz')\n",
    "        cols=['FILECODE','CDSRV','DTENC','NUTCKCLT','CDVTE','CDTCKVNT','QTVTE','MTVTE','ANNULATION','VLPDS','MTREM','Filler']\n",
    "        colspec_list=[(0,2),(2,7),(7,13),(13,20),(20,33),(33,36),(36,45),(45,54),(54,55),(55,64),(64,73),(74,80)]\n",
    "        FR1=pd.read_fwf(FR1,colspecs=colspec_list,names=cols,dtype=str,skiprows=2)\n",
    "    else:\n",
    "        FR1=cols=['FILECODE','CDSRV','DTENC','NUTCKCLT','CDVTE','CDTCKVNT','QTVTE','MTVTE','ANNULATION','VLPDS','MTREM','Filler']\n",
    "        colspec_list=[(0,2),(2,7),(7,13),(13,20),(20,33),(33,36),(36,45),(45,54),(54,55),(55,64),(64,73),(74,80)]\n",
    "        FR1=pd.read_fwf(mfullfilename,colspecs=colspec_list,names=cols,dtype=str,skiprows=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c775f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping \n",
    "temp = my_inds_dates['DTENC'].unique().tolist()\n",
    "temp1 = list(map(str, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"FILECODE\":pd.Series(dtype='object'),\n",
    "              \"CDSRVMAG\":pd.Series(dtype='int64'),\n",
    "              \"CDEANVTECFR\":pd.Series(dtype='int64')\n",
    "              ,\"DTDEBAPPCFR\":pd.Series(dtype='int64'),\"DTFINAPPCFR\":pd.Series(dtype='int64'),\"CDEANVTE\":pd.Series(dtype='int64'),\"VLPXVORI\":pd.Series(dtype='int64'),\"VLPXVREM\":pd.Series(dtype='int64'),\"Filler\":pd.Series(dtype='float64')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "except Exception as e:\n",
    "        exc_type,exc_obj,exc_tb = sys.exc_info()\n",
    "        logger.exception(f\"The {exc_type} exception  has occurred in the fucntion 'Loop_read_prh_id_conv'  at line :{exc_tb.tb_lineno}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML\n",
    "\n",
    "display(globals()[new_dataset+\"_char_changes\"].style.set_caption(\"CHAR CHANGE HISTORY CHECKS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a197f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys='HOUSEHOLD_ID~ROLE_CLIENT_CODE'\n",
    "#in_delimiter = '~'\n",
    "scan(keys,in_delimiter)\n",
    "[HOUSEHOLD_ID,ROLE_CLIENT_CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Proc_Sql(query,conn,dfs=[],names=[],to_df=False,drop=[]):\n",
    "    \n",
    "    \"\"\"Execute SQL queries of SAS in Python\n",
    "        query : query to be ran(query='' when just to convert dataframes to local db tables)\n",
    "        conn : server connection made\n",
    "        dfs : list of dataframes used in the query\n",
    "        names : names of dataframes in sql query in the order of dfs\n",
    "        to_df : True to return dataframe as a result of query\"\"\"\n",
    "    \n",
    "    #dropping columns in list drop\n",
    "    if drop != []:\n",
    "        for i in range(len(drop)):\n",
    "            conn.execute('DROP table IF EXISTS '+drop[i])\n",
    "    \n",
    "    #converting dataframes in dfs to sql table according to names\n",
    "    if dfs != [] and names != []:\n",
    "        for i in range(len(dfs)):\n",
    "            conn.execute('DROP table IF EXISTS '+names[i])\n",
    "            dfs[i].to_sql(names[i],conn,index=False)\n",
    "    \n",
    "    #return dataframe if to_df equals True\n",
    "    if to_df == True:\n",
    "        df = pd.read_sql(query,conn)\n",
    "        return df\n",
    "    else:\n",
    "        conn.execute(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x dos2unix -q ../data/&kw./bezirk_&kw..csv;\n",
    "\n",
    "    # importing input files\n",
    "    if platform == \"linux\" or platform == \"darwin\":\n",
    "        os.system(\"x dos2unix -q ../data/\"+str(kw)+\"/results_\"+str(kw)+\".csv\")\n",
    "    filename=\"Results_EA_\"+str(kw)+\".csv\"\n",
    "    results1=pd.read_csv(os.path.join(Input,filename),sep=';',encoding='ISO-8859-1')\n",
    "    logging.info(\"Input file fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soundex\n",
    "def soundex(query):\n",
    "    \"\"\"\n",
    "    https://en.wikipedia.org/wiki/Soundex\n",
    "    :param query:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        query = query.lower()\n",
    "        letters = [char for char in query ]\n",
    "    \n",
    "        # Step 1: Save the first letter. Remove all occurrences of a, e, i, o, u, y, h, w.\n",
    "    \n",
    "        # If query contains only 1 letter, return query+\"000\" (Refer step 5)\n",
    "        if len(query) == 1:\n",
    "            return query + \"000\"\n",
    "    \n",
    "        to_remove = ('a', 'e', 'i', 'o', 'u', 'y', 'h', 'w')\n",
    "    \n",
    "        first_letter = letters[0]\n",
    "        letters = letters[1:]\n",
    "        letters = [char for char in letters if char not in to_remove]\n",
    "    \n",
    "        if len(letters) == 0:\n",
    "#             print('try')\n",
    "            return first_letter + \"000\"\n",
    "    \n",
    "        # Step 2: Replace all consonants (include the first letter) with digits according to rules\n",
    "    \n",
    "        to_replace = {('b', 'f', 'p', 'v'): 1, ('c', 'g', 'j', 'k', 'q', 's', 'x', 'z'): 2,\n",
    "                      ('d', 't'): 3, ('l',): 4, ('m', 'n'): 5, ('r',): 6,' ':''}\n",
    "    \n",
    "        first_letter = [value if first_letter else first_letter for group, value in to_replace.items()\n",
    "                        if first_letter in group]\n",
    "        letters = [value if char else char\n",
    "                   for char in letters\n",
    "                   for group, value in to_replace.items()\n",
    "                   if char in group]\n",
    "    \n",
    "        # Step 3: Replace all adjacent same digits with one digit.\n",
    "        letters = [char for ind, char in enumerate(letters)\n",
    "                   if (ind == len(letters) - 1 or (ind+1 < len(letters) and char != letters[ind+1]))]\n",
    "    \n",
    "        # Step 4: If the saved letter’s digit is the same the resulting first digit, remove the digit (keep the letter)\n",
    "        if first_letter == letters[0]:\n",
    "            letters[0] = query[0]\n",
    "        else:\n",
    "            letters.insert(0, query[0])\n",
    "    \n",
    "        # Step 5: Append 3 zeros if result contains less than 3 digits.\n",
    "        # Remove all except first letter and 3 digits after it.\n",
    "    \n",
    "        first_letter = letters[0]\n",
    "        letters = letters[1:]\n",
    "    \n",
    "        letters = [char for char in letters if isinstance(char, int)][0:15]\n",
    "    \n",
    "        while len(letters) < 15:\n",
    "            letters.append(0)\n",
    "    \n",
    "        letters.insert(0, first_letter)\n",
    "    \n",
    "        string = \"\".join([str(l) for l in letters])\n",
    "#         print('try')\n",
    "        return string\n",
    "    \n",
    "    except:  \n",
    "        try:\n",
    "            query=str(float(query))\n",
    "            string = str(query[0])    \n",
    "            return string\n",
    "        except:\n",
    "            return query[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e02279",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst1:\n",
    "    db_conn.execute(i)\n",
    "\n",
    "lst2=[\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'ü','ue'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'Ü','Ue'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'ö','oe'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'Ö','Oe'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'ä','ae'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'Ä','Ae'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'ß','ss'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'str.','str'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'STR.','STR'))\"\"\",\n",
    "      \n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'strasse','str'))\"\"\",\n",
    "\n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'STRASSE','STR'))\"\"\",\n",
    "\n",
    "\"\"\"update lal_neu\n",
    "set street=upper(replace(street,'  ',' '))\"\"\"\n",
    "    ]\n",
    "\n",
    "for i in lst2:\n",
    "    db_conn.execute(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging part\n",
    "import logging\n",
    "logger=logging.getLogger()\n",
    "\n",
    "logging.basicConfig(filename='ldl_dataprep_2015.log',\n",
    "                            filemode='w',\n",
    "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                            datefmt='%d-%b-%y %H:%M:%S',\n",
    "                            level=logging.DEBUG)\n",
    "logging.info(\"Execution starts\")\n",
    "\n",
    "logger.info('Value of CounProj is %s' %CounProj)\n",
    "\n",
    " logging.info(\"The input table %s has %s rows and %s columns.\" %(rawd,len(rawdf.axes[0]),len(rawdf.axes[1])))\n",
    "rawdf.columns=rawdf.columns.str.lower()\n",
    "\n",
    "\n",
    "logging.info(f\"Table itm_{str(weekno)} has %s  rows and %s columns\" %(len(itm_.axes[0]),len(itm_.axes[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbf3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if compbl(D_68090||D_68092) ne \"\" then Pack_Size=compbl(compbl(D_68090)||\" \"||compbl(D_68092));\n",
    "    else if compbl(D_67658) ne \"\" then Pack_Size=(compbl(D_67658));\n",
    "    else if compbl(D_67659) ne \"\" then Pack_Size=(compbl(D_67659));\n",
    "    else if compbl(D_67660) ne \"\" then Pack_Size=(compbl(D_67660));\n",
    "    else if compbl(D_67661) ne \"\" then Pack_Size=(compbl(D_67661));\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# applying conditions on columns of dataframe extractimdb                          \n",
    "extractimdb['Pack_Size']=np.vectorize(concatenation)(extractimdb['D_68090'], extractimdb['D_68092'])\n",
    "\n",
    "extractimdb['Pack_Size']=np.vectorize(concatenation_1)(extractimdb['D_67658'])\n",
    "extractimdb['Pack_Size']=np.vectorize(concatenation_1)(extractimdb['D_67659'])\n",
    "extractimdb['Pack_Size']=np.vectorize(concatenation_1)(extractimdb['D_67660'])\n",
    "extractimdb['Pack_Size']=np.vectorize(concatenation_1)(extractimdb['D_67661'])                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c844010",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isaper['count(distinct week_id)'].all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ccbe6",
   "metadata": {},
   "source": [
    "##### Should use path as ldata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97544b77",
   "metadata": {},
   "source": [
    "# Deleting codes\n",
    "del cnan_total\n",
    "del cnan_promo\n",
    "del cnan_non_promo   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419af10a",
   "metadata": {},
   "source": [
    "# DATA Cleaning\n",
    "    cnan['nlig']=cnan['nlig'].astype('float64')\n",
    "    cnan['cnan']=cnan['cnan'].astype('float64')\n",
    "    cnan=cnan[['nlig','cnan', 'conv1']]\n",
    "    \n",
    "    wcontract['nlig']=wcontract['nlig'].astype('float64')\n",
    "    wcontract['dts']=wcontract['dts'].fillna(0)\n",
    "    wcontract['dts']=wcontract['dts'].astype(int)\n",
    "    wcontract['dts']=wcontract['dts'].astype(str)\n",
    "    wcontract=wcontract[['nlig','line_desc','conv','mother','level','valid','dts','prh_id','dupli','xf_master']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('mc_compressg.py').read())\n",
    "exec(open('info.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e24e7",
   "metadata": {},
   "source": [
    "# Creating Functions\n",
    "def delete(x):\n",
    "                '''This function is to delete the existing tables in the sql database'''\n",
    "                db_conn.execute('DROP table IF EXISTS ' +x)\n",
    "def to_sql(x,y):\n",
    "        '''This function is to write the dataframe to sql database'''\n",
    "        x.to_sql(y,db_conn)\n",
    "        \n",
    "def read_file(filepath):\n",
    "    '''This function is used to read the input file with tab seperated'''\n",
    "    data=pd.read_excel(filepath)\n",
    "    return data\n",
    "\n",
    "def export_file(filename,filepath):\n",
    "    '''This function is used to export/write the data into output file with tab seperated'''\n",
    "    filename.to_excel(filepath,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=pd.DataFrame({\"Orrange\":[12,233,233,356,4,5,6],\n",
    "             \"Chikku\":[7,2,0,7,0,6,7],\n",
    "               \"Badam\":[7,2,3,7,5,6,9]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b2554",
   "metadata": {},
   "source": [
    "# Removing the files\n",
    "file_wContract_orig= (lData+'wContract_orig.csv')\n",
    "if os.path.exists(file_wContract_orig):\n",
    "    os.remove(file_wContract_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d35eb",
   "metadata": {},
   "source": [
    "# SQL\n",
    "INP_FIL.to_sql('INP_FIL',db_conn,if_exists='replace',chunksize=40)\n",
    "\n",
    "chain_working=pd.read_sql_query(f'''select * from chain_working where ShopTYP = {\"'\" + shoptyp  + \"'\"} and PerFrom = {\"'\" + perfromN  + \"'\"}''',db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dceee8f",
   "metadata": {},
   "source": [
    "# Merging with indicator\n",
    "wcontract=wcontract.merge(xfMASTER,how=\"outer\",indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865da542",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''create table LAL_alt as \n",
    "select ash, TDLinx, allocno, adrname, adrstr, zipcode, city, salesarea, fps, typ, comment\n",
    "from address_universe_working\n",
    "where \n",
    "perfrom=:perfrom\n",
    "and allocno between :allocno1 and :allocno2\n",
    "\n",
    "order by zipcode;'''\n",
    "\n",
    "LAL_alt=db_conn.execute(query,typ=typ,perfrom=config_file_125.perfrom,allocno1=config_file_125.allocno1,allocno2=config_file_125.allocno2,allocnot=config_file_125.allocnot)\n",
    "LAL_alt=pd.DataFrame(pd.read_sql_query('select * from LAL_alt', db_conn))\n",
    "LAL_alt=LAL_alt[(LAL_alt['Typ'].isin(typ)) & ~(LAL_alt['Typ'].isin(config_file_125.allocnot))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e052e6e",
   "metadata": {},
   "source": [
    "# To Frame\n",
    "cnan_reference.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bec70e",
   "metadata": {},
   "source": [
    "# removing duplicated columns\n",
    "adr_chain= adr_chain.loc[:,~adr_chain.columns.duplicated()]\n",
    "\n",
    "['results_'+str(region[i])].loc[:,~globals()['results_'+str(region[i])].columns.duplicated()]\n",
    "\n",
    "cnan_reference.drop_duplicates(subset='cnan',keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e992c2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'substr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18236/861499956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;34m\"Hailofleenn\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'substr'"
     ]
    }
   ],
   "source": [
    "# uncomment these lines to establish connection with MSCI server\n",
    "#import pyodbc\n",
    "#conn = pyodbc.connect('DRIVER={SQL Server};''SERVER=LHRWINSQLP053.enterprisenet.org;''DATABASE=MSCI_UNIVERSE;''Trusted_Connection=yes;')\n",
    "#cur = conn.cursor()\n",
    "\n",
    "#should do at the end\n",
    "#conn.commit()\n",
    "libname univ ODBC NOPROMPT=\"Driver={SQL Server};Server=LHRWINSQLP053;\n",
    "Database=MSCI_UNIVERSE;\" Schema=DBO;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strfTime\n",
    "%let t = %sysfunc( datetime(), datetime14. );\n",
    "%put ***** TIME CHECK AT START: &t  *****;\n",
    "time.strftime(\"***** TIME CHECK AT START: %d%b%y:%H:%M  *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc04ca2",
   "metadata": {},
   "source": [
    "# Dropping the duplicates columns\n",
    "apo_area = apo_area.loc[:,~apo_area.columns.duplicated()]\n",
    "\n",
    "cols_to_keep=['ac_shop','NC_NAN_KEY', 'pur_date_yyyyww', 'promo']\n",
    "EF_DATA.drop(EF_DATA.columns.difference(cols_to_keep), axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7138a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "        CONNECT TO oracle AS nrsp (path=VALPRD01 password=NRSP_GUEST user=NRSP_GUEST);\n",
    "        CREATE TABLE &dsout AS\n",
    "          SELECT * FROM CONNECTION TO nrsp\n",
    "            (select FHP_FDS_ID, FHP_ITM_ID as itm_id\n",
    "             from NRSP_RES.VRAG_FLAT_HIE_PRD\n",
    "             where FHP_FDS_ID=&FdsId and fhp_phi_id=fhp_cons_phi_id\n",
    "             )\n",
    "        ;\n",
    "        DISCONNECT FROM nrsp;\n",
    "        select count(FHP_FDS_ID) into:rdVALPRD_TAG from &dsout;\n",
    "      quit;  \n",
    "\n",
    "\n",
    "cx_Oracle.init_oracle_client(lib_dir=r\"C:\\Users\\2104384\\Downloads\\instantclient-basic-windows.x64-19.12.0.0.0dbru\\instantclient_19_12\")\n",
    "        dsn_tns = cx_Oracle.makedsn('oravalprdscan', '1535', service_name='VALPRDMDR')\n",
    "        conn = cx_Oracle.connect(user='NRSP_GUEST', password='NRSP_GUEST', dsn=dsn_tns)\n",
    "        c = conn.cursor()\n",
    "        \n",
    "        query='''select FHP_FDS_ID, FHP_ITM_ID as itm_id\n",
    "                      from NRSP_RES.VRAG_FLAT_HIE_PRD where FHP_FDS_ID={} AND fhp_phi_id=fhp_cons_phi_id'''.format(FdsId)\n",
    "        a= pd.read_sql_query(query,con = conn)\n",
    "        a\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d486390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "Load_Df_Type = {'FILECODE': str,\n",
    "                   'CDOFRFDL': str,\n",
    "                   'CTOFRFDL': str,\n",
    "                   'LBOFRFDL': str,\n",
    "                   'LBOFRTCK': str,\n",
    "                   'DDOFRFDL': str,\n",
    "                   'Filler1': str,\n",
    "                   'DFOFRFDL': str,\n",
    "                   'Filler2': str,\n",
    "                   'NBATTRIB': str,\n",
    "                   'Filler3': str,\n",
    "                   'ORSENS': str,\n",
    "                   'LBCMG': str,\n",
    "                   'MTREM': float,\n",
    "                   'PCREM': float,\n",
    "                   'NBPTS': float,\n",
    "                   'Filler4': str,\n",
    "                   'TYPMEC': str,\n",
    "                   'CDCUPEXT': str,\n",
    "                   'CDCATCMG': str,\n",
    "                   'CDSGM': str,\n",
    "                   'CDRGPEXT': str,\n",
    "                   'Filler5': str,\n",
    "                   'CDCMG': str,\n",
    "                   'Filler6': str}\n",
    "\n",
    "converters_list = {'NBATTRIB':lambda x : str(x),\n",
    "                              'ORSENS':lambda x : str(x),\n",
    "                              'TYPMEC':lambda x : str(x)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FF = f'{minlib}FR1.txt'\n",
    "    Load_Df_columns = ['FILECODE','CDOFRFDL','CTOFRFDL','LBOFRFDL','LBOFRTCK','DDOFRFDL','Filler1',\n",
    "                      'DFOFRFDL','Filler2','NBATTRIB','Filler3','ORSENS','LBCMG','MTREM','PCREM','NBPTS',\n",
    "                      'Filler4','TYPMEC','CDCUPEXT','CDCATCMG','CDSGM','CDRGPEXT','Filler5','CDCMG','Filler6']\n",
    "\n",
    "    colspecs_list = [(0,2),(2,10),(10,12),(12,47),(47,67),(67,75),(75,79),(79,87),(87,97),(97,99),\n",
    "                     (99,101),(101,102),(102,132),(132,141),(141,146),(146,151),(151,167),(167,168),\n",
    "                     (168,183),(183,186),(186,196),(196,211),(211,302),(302,317),(317,320)]\n",
    "    Load_Df=pd.read_fwf(FF,colspecs=colspecs_list,names  = Load_Df_columns,converters = converters_list ,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#or removing mulpiple spaces\n",
    "extractimdb['Flavor']=extractimdb['Flavor'].replace('\\s+',' ',regex=True)\n",
    "\n",
    "extractimdb['Pack_Size_STD']=extractimdb['Pack_Size_STD'].str.rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e4b0fc",
   "metadata": {},
   "source": [
    "# Declaring Schema of the DataFrame\n",
    "\n",
    "mdataset_name_schema = pd.DataFrame{'FILECODE': pd.Series(dtype='object'),\n",
    "                   'CDOFRFDL': pd.Series(dtype='object'),\n",
    "                   'CTOFRFDL': pd.Series(dtype='object'),\n",
    "                   'LBOFRFDL': pd.Series(dtype='object'),\n",
    "                   'LBOFRTCK': pd.Series(dtype='object'),\n",
    "                   'DDOFRFDL': pd.Series(dtype='float64'),\n",
    "                   'Filler1': pd.Series(dtype='object'),\n",
    "                   'DFOFRFDL': pd.Series(dtype='float64'),\n",
    "                   'Filler2': pd.Series(dtype='object'),\n",
    "                   'NBATTRIB': pd.Series(dtype='object'),\n",
    "                   'Filler3': pd.Series(dtype='object'),\n",
    "                   'ORSENS': pd.Series(dtype='object'),\n",
    "                   'LBCMG': pd.Series(dtype='object'),\n",
    "                   'MTREM': pd.Series(dtype='float64'),\n",
    "                   'PCREM': pd.Series(dtype='float64'),\n",
    "                   'NBPTS': pd.Series(dtype='float64'),\n",
    "                   'Filler4': pd.Series(dtype='object'),\n",
    "                   'TYPMEC': pd.Series(dtype='object'),\n",
    "                   'CDCUPEXT': pd.Series(dtype='object'),\n",
    "                   'CDCATCMG': pd.Series(dtype='object'),\n",
    "                   'CDSGM': pd.Series(dtype='object'),\n",
    "                   'CDRGPEXT': pd.Series(dtype='object'),\n",
    "                   'Filler5': pd.Series(dtype='object'),\n",
    "                   'CDCMG': pd.Series(dtype='object'),\n",
    "                   'Filler6': pd.Series(dtype='object')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139426d",
   "metadata": {},
   "source": [
    "# Printing Log\n",
    "#proc printto\n",
    " log=\"str(outdest)+/update_comparable_flags_202150_+str(daydate)+.log\"     print=\"str(outdest)+/update_comparable_flags_202150_+str(daydate)+.lst\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839b7da",
   "metadata": {},
   "source": [
    "# used to fetch the records from db_conn.exe\n",
    "req_wks=db_conn.execute(\"\"\"select count(*) from period_week_seq where week_id between {ty_fr} and {ty_to}\"\"\".format(ty_fr=ty_fr,ty_to=ty_to)).fetchall()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAS\n",
    "PROC SQL;\n",
    "connect to ODBC as prueba (user=NRSP_OPSES password=NRSP_OPSES datasrc=VALPRD01);\n",
    "CREATE TABLE ILIST1 AS\n",
    "SELECT *\n",
    "from connection to prueba\n",
    "(select distinct phi_id, TRIM('P') || TRIM(TO_CHAR(A.HIER_ID,'000000000000000000')) || TRIM(TO_CHAR(A.NODE_ID)) as nodo, a.* \n",
    "from nrsp_v.vrag_fds_flat_hie_prd a, nrsp_v.trag_ps_hierarchy_prd b\n",
    "where fds_id = 1175260\n",
    "and phi_fds_id = fds_id and hier_id = phi_id);\n",
    "disconnect from prueba;\n",
    "QUIT;\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ea4a3",
   "metadata": {},
   "source": [
    "# Python connection\n",
    "#init_oracle_client , should be run only first time to initiate the oracle client after that comment it\n",
    "#cx_Oracle.init_oracle_client(lib_dir=r\"C:\\Users\\1928695\\Downloads\\instantclient-basic-windows.x64-19.12.0.0.0dbru (1) (1)\\instantclient_19_12\")\n",
    "dsn_tns = cx_Oracle.makedsn('oravalprdscan.enterprisenet.org', '1535', service_name='VALPRDMDR')\n",
    "conn = cx_Oracle.connect(user='NRSP_OPSES', password='NRSP_OPSES', dsn=dsn_tns)\n",
    "cur = conn.cursor()\n",
    "cur.prefetchrows = 50000 #you can increase the prefetch size and arraysize for fast execution, depends on hardware, mainly used to reduce memory load at a time only 50000 records are fetched\n",
    "cur.arraysize = 50000    #you can increase the prefetch size and arraysize for fast execution, depends on hardware\n",
    "dlist1=[]\n",
    "for row in cur.execute(\"(select distinct phi_id, TRIM('P') || TRIM(TO_CHAR(A.HIER_ID,'000000000000000000')) || TRIM(TO_CHAR(A.NODE_ID)) as nodo, a.* from nrsp_v.vrag_fds_flat_hie_prd a, nrsp_v.trag_ps_hierarchy_prd b where fds_id = 1175260 and phi_fds_id = fds_id and hier_id = phi_id)\"):\n",
    "    dlist1.append(row)\n",
    "\n",
    "ILIST1=pd.DataFrame(dlist1,columns=['PHI_ID','NODO','FDS_ID','HIER_ID','NODE_ID','LEV','NAN_KEY']) \n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8712d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranwrd\n",
    "SAS:-    labl=tranwrd(labl, \",\" ,  \"/\");\n",
    "Python:- lih['labl'] = lih['labl'].str.replace(\",\",\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading path from config\n",
    "address_universe_working=pd.read_excel(os.path.join(config.path,config.address_universe_working))\n",
    "IN_input1=pd.read_excel(os.path.join(config.path,config.oldnew_gam_td))\n",
    "IN_input2=pd.read_excel(os.path.join(config.path,config.compare_remains))\n",
    "\n",
    "\n",
    "\n",
    "pathdwh=os.path.join(\"C:\",\"\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lData=os.path.join('..','data'+os.sep) \n",
    "lPurch=os.path.join('..','data','extract'+os.sep) \n",
    "lproEF =os.path.join('department','model','dwh','ncps','ncps_2.1','input','FR','EF_data'+os.sep)\n",
    "LIN=os.path.join('..','data'+os.sep) \n",
    "\n",
    "\n",
    "BAR_APO=os.path.join(pathdwh,\"Users\",\"1992800\",\"Downloads\",\"Census_ReweVSFil+P.xlsx\")\n",
    "outfile1=os.path.join(pathdwh,\"My_Sas_Exit\",\"UC125\",\"new\"+filename)\n",
    "outfile2=os.path.join(pathdwh,\"My_Sas_Exit\",\"UC125\",\"delete\"+filename)\n",
    "outfile3=os.path.join(pathdwh,\"My_Sas_Exit\",\"UC125\",\"oldnew\"+filename)\n",
    "outfile4=os.path.join(pathdwh,\"My_Sas_Exit\",\"UC125\",\"compare_remains\"+filename)\n",
    "address_universe_working=os.path.join(pathdwh,\"Users\",\"1992800\",\"Downloads\",\"address_universe_working.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_col=[0] removes unnamed index column\n",
    "cnan_orig=pd.read_csv(lData+\"cnan_orig.csv\",index_col=[0],dtype={'conv1': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d903294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DateTime formaat\n",
    "    Load_Df['DDOFRFDL'] = pd.to_datetime(Load_Df['DDOFRFDL'],errors = 'coerce',format=\"%Y-%m-%d\").dt.strftime(\"%y-%m-%d\")\n",
    "    Load_Df['DFOFRFDL'] = pd.to_datetime(Load_Df['DFOFRFDL'], errors = 'coerce',format=\"%Y-%m-%d\").dt.strftime(\"%y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce7df2",
   "metadata": {},
   "source": [
    "# Label\n",
    "a=pd.DataFrame({\"A\":[12,3,44,4]})\n",
    "a.style.set_caption(\"list of files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1230dc1",
   "metadata": {},
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import portalocker\n",
    "import gc\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from termcolor import colored, cprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364dc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import exc\n",
    "\n",
    "db_path=\"C:\\\\Users\\\\1949524\\\\TEST_DB\\\\\"\n",
    "db_name=\"Test333.db\"\n",
    "db_conn = create_engine(r\"sqlite:///\"+db_path+db_name,echo=False).connect()\n",
    "\n",
    "db_conn=create_engine('sqlite://',echo=False)\n",
    "db_conn=create_engine(r\"sqlite:///\",echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7161f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop with fetching specific value  from column\n",
    "for k in range(1,len(metadat_liam123)):\n",
    "    fdict['varname'+str(k)] = metadat_liam123.at[k,'NAME'].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditions\n",
    "            derived_facts_2.loc[ derived_facts_2['CATEGORY'] == \"TOTAL FMCG\",'UWG_VALUE_SHARE']=[np.nan]\n",
    "            derived_facts_2.loc[ derived_facts_2['CATEGORY'] != \"TOTAL FMCG\",'UWG_VALUE_SHARE']=derived_facts_2['VALUE']/(derived_facts_2['FMCG_VALUE']*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f777138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAP FUNCTION\n",
    "        globals()[country+'_'+'ldl'+'_liam1_2'].columns = map(str.upper, globals()[country+'_'+'ldl'+'_liam1_2'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a94b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating folders\n",
    "if os.path.isdir(path1)==False:\n",
    "    os.mkdir(path1)\n",
    "if os.path.isdir(path2)==False:\n",
    "    os.mkdir(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATETIME\n",
    "todaysdate = datetime.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wELEMTF['csenddate'] = np.select(conditions, choices,default=wELEMTF['csenddate_'].apply(rdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ad7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUMMARY\n",
    "\n",
    "# Calculate the sum of the adjusted sales over all households\n",
    "        df2=PURCHASE_SHAREFACTOR.groupby(['XX_GROUP', 'CODE', 'CATEGORY','PROMO_GROUP']).agg({'VALUE_ADJUSTED':'sum'}).reset_index()\n",
    "        df2['_type_']=1\n",
    "        df=PURCHASE_SHAREFACTOR.groupby(['XX_GROUP', 'CODE', 'CATEGORY']).agg({'VALUE_ADJUSTED':'sum'}).reset_index()\n",
    "        df['PROMO_GROUP']=''\n",
    "        df['_type_']=0\n",
    "        df=df[['XX_GROUP', 'CODE', 'CATEGORY','PROMO_GROUP','VALUE_ADJUSTED','_type_']]\n",
    "        PURCHASE_SHAREFACTOR_SUM=df2.append(df)\n",
    "        del df2,df\n",
    "        PURCHASE_SHAREFACTOR_SUM=PURCHASE_SHAREFACTOR_SUM.sort_values(by=['XX_GROUP', 'CODE', 'CATEGORY','_type_'])\n",
    "        PURCHASE_SHAREFACTOR_SUM.reset_index(drop=True)\n",
    "        PURCHASE_SHAREFACTOR_SUM=PURCHASE_SHAREFACTOR_SUM.rename(columns={'VALUE_ADJUSTED':'TOTAL_VALUE_ADJUSTED'})\n",
    "        logger.info(\"Table PURCHASE_SHAREFACTOR_SUM has \"+str(len(PURCHASE_SHAREFACTOR_SUM.axes[0]))+\" rows and \"+str(len(PURCHASE_SHAREFACTOR_SUM.axes[1]))+\" columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib Monday_Summary format=ddmmyy10.\n",
    "\n",
    "NCPS_SUMMARY['monday']=  NCPS_SUMMARY['Monday_Summary']\n",
    "NCPS_SUMMARY['delta']= pd.to_timedelta(NCPS_SUMMARY['Monday_Summary'], unit='d')\n",
    "NCPS_SUMMARY['d0']=date(1960,1,1)\n",
    "NCPS_SUMMARY['d0']= pd.to_datetime(NCPS_SUMMARY['d0'])\n",
    "NCPS_SUMMARY['Monday_Summary']=NCPS_SUMMARY['d0']+NCPS_SUMMARY['delta'] \n",
    "NCPS_SUMMARY['Monday_Summary']=pd.to_datetime(NCPS_SUMMARY['Monday_Summary']).dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"*************Execution starts \"+(datetime.now()).strftime(\"%H:%M:%S\")+\"*************\\n\\n\")\n",
    "cx_Oracle.init_oracle_client(lib_dir=r\"C:\\Users\\1949524\\OneDrive - TCS COM PROD\\Documents\\instantclient_19_12\")\n",
    "dsn_tns = cx_Oracle.makedsn(config.host, config.port, service_name=config.service)\n",
    "conn = cx_Oracle.connect(user=config.user_id, password=config.passwrod, dsn=dsn_tns)\n",
    "c = conn.cursor()\n",
    "c.arraysize=config.size\n",
    "c.prefetchrows=config.size\n",
    "df_tuple=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebdadf9",
   "metadata": {},
   "source": [
    "# Accessing Config file in .ini format. \n",
    "file='UC125_02_LEH_ALL_input.ini'\n",
    "config=configparser.RawConfigParser()\n",
    "config.read(file)\n",
    "\n",
    "\n",
    "file='uc125_02_chain_compare_acv_output_LEH_neu.ini'\n",
    "config=ConfigParser()\n",
    "config.read(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb8257",
   "metadata": {},
   "source": [
    "# Summary\n",
    "proc summary data=anz;\n",
    "class keyaccount  typ;\n",
    "var ANZ CV;\n",
    "output out=anzahl (DROP=_TYPE_ _FREQ_)SUM= ;\n",
    "run;\n",
    "\n",
    "anzahl=anz.groupby(['KeyAccount', 'Typ']).agg({'ANZ':sum,'CV':sum}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#creating dataframe summ_cust_types by taking count of the columns \"Customer_Type_ID\", \"Customer_Type_Desc\"\n",
    "summ_cust_types=globals()['customer_ '+str(per)].groupby([\"Customer_Type_ID\",\"Customer_Type_Desc\"]).size().reset_index(name=\"COUNT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency\n",
    "def proc_freq(ds,var1):\n",
    "    '''function to return the frequency of the dataframe '''\n",
    "\n",
    "    f=ds[var1].value_counts(dropna=False)\n",
    "    p=ds[var1].value_counts(dropna=False, normalize=True)\n",
    "    df=pd.concat([f,p], axis=1, keys=['frequency', 'percent'])\n",
    "    df['percent']=df['percent']*100\n",
    "    df[\"cumfrequency\"] = df[\"frequency\"].cumsum()\n",
    "    df[\"cumpercent\"] = df[\"percent\"].cumsum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69114ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML Codes\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"ADRFILE: ANZ + UMSATZ \" +str(perfromN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221a0da",
   "metadata": {},
   "source": [
    "# Dynamic DatasetsLooping with global\n",
    "proc sql;\n",
    "\tselect distinct key_acc into :lst_key_acc separated by \" \" from finput.validation;\n",
    "\n",
    "\n",
    "%do i = 1 %to %sysfunc(countw(&lst_key_acc.));\n",
    "\n",
    "\t%let key_acc_sub = %scan(&lst_key_acc., &i.);\n",
    "\tdata val_&key_acc_sub.;\n",
    "\t\tset finput.validation;\n",
    "\t\tif key_acc eq &key_acc_sub.;\n",
    "\trun;\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(lst_key_acc.split())) :\n",
    "    key_acc_sub=str(int(float(lst_key_acc.split()[i])))\n",
    "    globals()['val_'+str(key_acc_sub)]=validation\n",
    "    globals()['val_'+str(key_acc_sub)]=globals()['val_'+str(key_acc_sub)].loc[globals()['val_'+str(key_acc_sub)][\"key_acc\"]==float(key_acc_sub)]\n",
    "    globals()['val_'+str(key_acc_sub)][\"key_acc\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1795eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "VMKSMG=XXFIL.copy(deep=True)\n",
    "VMKSMG['RAT']=None\n",
    "VMKSMG=VMKSMG.apply(func_VMKSMG,axis=1)\n",
    "# VMKSMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582902c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "comma20. \n",
    "selecr * , al_cps_p1 format = comma20. as val_cps_p1_orig\n",
    "\n",
    "alignment.loc[:, \"val_cps_p1_orig\"] =alignment[\"val_cps_p1_orig\"].map('{:,d}'.format)\n",
    "\n",
    "converts 96794945 96,794,945"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a79e2",
   "metadata": {},
   "source": [
    "# for changing the NONE to empty space\n",
    "mask = address_universe_working.applymap(lambda x: x is None)\n",
    "cols = address_universe_working.columns[(mask).any()]\n",
    "for col in address_universe_working[cols]:\n",
    "    address_universe_working.loc[mask[col], col] = ''\n",
    "address_universe_working.to_sql('address_universe_working',db_conn,index=False,if_exists='replace')\n",
    "address_universe_working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e833e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditions\n",
    "conditions = [(HOS['ZUGNR']==3197),(HOS['ZUGNR']==6191),(HOS['ZUGNR']==6193),(HOS['ZUGNR']==6194)]\n",
    "choices = [1111114,1111116,1111115,1111113]\n",
    "HOS['RAT'] = np.select(conditions, choices,default = 99999 )\n",
    "\n",
    "DATA HOS; \n",
    " SET XXFIL;\n",
    "\n",
    "  IF TYP IN ('DRO')             THEN DO;\n",
    "            IF ZUGNR=3197                        THEN RAT=1111114;\n",
    "       ELSE IF ZUGNR=6191                        THEN RAT=1111116;\n",
    "       ELSE IF ZUGNR=6193                        THEN RAT=1111115;\n",
    "       ELSE IF ZUGNR=6194                        THEN RAT=1111113;\n",
    "\t   ELSE                                           RAT=99999;\n",
    "  OUTPUT;\n",
    "  END;\n",
    "    \n",
    "    \n",
    "#Using Loc:-    \n",
    "if xf_master0 gt 0 \n",
    "       then xf_master = xf_master0;\n",
    "       else xf_master = nlig - mod(nlig,&coef.);    \n",
    "        \n",
    "wcontract.loc[wcontract['xf_master0'] > 0, 'xf_master'] = wcontract['xf_master0']\n",
    "    wcontract.loc[wcontract['xf_master0'] <= 0, 'xf_master'] = (wcontract['nlig']-(wcontract['nlig']%coef))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "PURCHASE_SHAREFACTOR=pd.merge(ALL_CPS_SALES,RMS_CPS_COMP_MARKET[['CODE', 'CATEGORY', 'PROMO_GROUP', 'UPLIFT', 'SHARE_FACTOR']],on=['CODE', 'PROMO_GROUP'],how='left', suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776566e4",
   "metadata": {},
   "source": [
    "# NP.Where\n",
    "cnan['conv1'] = np.where(cnan['conv1']<=0,0,cnan['conv1'])\n",
    "                             condition,value,column\n",
    "np.where(anz['Typ'].isin(lst),'LEH','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47440985",
   "metadata": {},
   "source": [
    "# Proc Tabulate\n",
    "PROC TABULATE DATA=anzahl     F=6.  FORMCHAR='!----!+!---'    ; \n",
    " TITLE3 \"ADRFILE: ANZ + UMSATZ      &perfromN     \";\n",
    " CLASS keyaccount typ1 shoptyp;                               \n",
    " VAR ANZ cv      ;                                   \n",
    " TABLE  keyaccount ALL , typ1*(ANZ cv) shoptyp*(ANZ cv)               \n",
    "         /RTS=15   BOX='LEH'          ;                     \n",
    "                                                              \n",
    "   KEYLABEL  SUM=' '   ;    \n",
    " \n",
    "run; \n",
    "\n",
    "\n",
    "anzahl1=anzahl.pivot_table(index=[\"KeyAccount\",\"typ1\"],aggfunc={'anz':'sum','CV':'sum'}, fill_value=0).reset_index()\n",
    "anzahl2={'KeyAccount':'ALL','anz':anzahl1['anz'].sum(),'CV':anzahl1['CV'].sum()}\n",
    "anzahl1=anzahl1.append(anzahl2,ignore_index=True)\n",
    "anzahl1=anzahl1.set_index(['KeyAccount',\"typ1\"])\n",
    "anzahl1.style.set_caption(\"ADRFILE: ANZ + UMSATZ  21019\")\n",
    "\n",
    "anzahl3=anzahl.pivot_table(index=[\"KeyAccount\",\"Shoptyp\"],aggfunc={'anz':'sum','CV':'sum'}, fill_value=0).reset_index()\n",
    "anzahl2={'KeyAccount':'ALL','anz':anzahl3['anz'].sum(),'CV':anzahl3['CV'].sum()}\n",
    "anzahl3=anzahl3.append(anzahl2,ignore_index=True)\n",
    "anzahl3=anzahl3.set_index(['KeyAccount',\"Shoptyp\"])\n",
    "anzahl3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TODOEANNANPC_1=TODOEANNANPC.pivot_table(index=['PC_DESCRIPCION'],aggfunc={'UNIDADES':'sum','VALOR':'sum','REF':'sum'}, fill_value=0).reset_index()\n",
    "TODOEANNANPC_2={'PC_DESCRIPCION':'ALL','UNIDADES':TODOEANNANPC_1['UNIDADES'].sum(),'VALOR':TODOEANNANPC_1['VALOR'].sum(),'REF':TODOEANNANPC_1['REF'].sum()}\n",
    "TODOEANNANPC1=TODOEANNANPC_1.append(TODOEANNANPC_2,ignore_index=True)\n",
    "TODOEANNANPCTAB=TODOEANNANPC1.set_index(['PC_DESCRIPCION'])\n",
    "writetolog('Tabular data for (PC_DESCRIPCION ALL):','LaCooperativaCad')\n",
    "logger.info(logdf(\"TODOEANNANPCTAB\",TODOEANNANPCTAB))\n",
    "del TODOEANNANPC_1,TODOEANNANPC_2,TODOEANNANPC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09777555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting\n",
    "writer = pd.ExcelWriter(ALL_1_output_file, engine='xlsxwriter')\n",
    "ALL_1.to_excel(writer,index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d7ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency\n",
    "proc freq noprint; table nlig / out=cntCNAN;\n",
    "cntCNAN=cnan.groupby(['nlig']).size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe0777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose\n",
    "\n",
    "proc transpose data=prd_focus out=_prd_focus prefix=line; by nc_nan_key;\n",
    "\twhere line in(&line_focus);\n",
    "\tvar uni; \n",
    "\tid line;\n",
    "run;\n",
    "\n",
    "\n",
    "\n",
    " _prd_focus=(prd_focus.set_index(['nc_nan_key']).pivot(columns='line')['uni'].add_prefix('line').reset_index()).rename_axis(None, axis=1)\n",
    "\n",
    "    \n",
    " proc transpose data=ACHAT01 out=ACHAT01 prefix=l suffix=p&per;\n",
    "       var SD;\n",
    "       by ac_hhold mbdR nligNLR;\n",
    "       id level; \n",
    "    \n",
    "# Performing Transpose\n",
    "ACHAT01=(ACHAT01.set_index(['AC_HHOLD','mbdR','nligNLR']).pivot(columns='level')['SD'].add_prefix('l').add_suffix('p'+str(per)).reset_index()).rename_axis(None, axis=1)\n",
    "logger.info(\"Table ACHAT01 has {0} rows and {1} columns\".format(len(ACHAT01.axes[0]),len(ACHAT01.axes[1])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "proc transpose data = bc_char_nacs out = usref.bc_char_nacs_T_0(drop = _NAME_ _LABEL_);\n",
    "by upc;\n",
    "ID BC_CHAR_TYPE;\n",
    "var BC_CHAR_VALUE ;\n",
    "run;\n",
    "\n",
    "# Transpose the dataframe\n",
    "bc_char_nacs_T_0=bc_char_nacs.pivot(index='UPC',columns='BC_CHAR_TYPE',values='BC_CHAR_VALUE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
